%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{COMP9417: Homework Set \#2} % Title of the assignment

\author{z5113817} % Author name and email address

\date{University of New South Wales --- \today} % University, school and/or department name(s) and a date

\newcommand\simplelrg{\hat{\beta}_{1} = \frac{\bar{XY} - \bar{X}\bar{Y}}{\bar{(X^2)} - (\bar{X})^2}}

\newcommand\sumlrg{\frac{1}{n}\sum_{i=1}^{n}(}
\newcommand\expandedlrg{\hat{\beta}_{1} = \frac{\sumlrg{}X_{i} - \bar{X})(Y_{i} - \bar{Y})}{\sumlrg{}X_{i} - \bar{X})^2}}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	Main Contents
%----------------------------------------------------------------------------------------

\section*{Question 1}

\subsection*{a} 
The possible values for both \(y_{i}\) and \(\tilde{y}_{i}\) are binary. Even though they have different values
(\(y_{i}\in\{0,1\}\) and \(\tilde{y}_{i}\in\{-1,1\}\)), the objective of each logistic regression implementation 
is to divide the dataset into 2 classifications. Because of this, the actual value that each classification has 
will not affect the parameters that the regression is attempting to optimise (\((\hat{\beta}_{0}, \hat{\beta})\) and
\((\hat{w}, \hat{c})\)). Therefore the solutions for the parameters being minimised by each regression will be the 
same.

\(C\) is a hyper-parameter that adjusts the sensitivity that the model has to its coefficients. Compared with the
standard LASSO parameter \(\lambda\), \(C\) is a multiple of the Loss function whereas \(\lambda\) is a multiple
of the Penalty.

\subsection*{b}
\label{1b}

Boxplot of testing accuracy for each value of C:

\includegraphics[scale=0.9]{q1b_boxplot.png}

The value of C returning best results: \textbf{0.18794747474747472}

The testing accuracy of this model: \textbf{76\%}

\newpage

\subsection*{c}

\textbf{From GridSearchCV:}

The value of C returning best results: \textbf{0.012219191919191918}

The testing accuracy of this model: \textbf{75.2\%}

In our answer for b \ref{1b}, we determined the "best" value of C as the value which corresponded to the 
average lowest \emph{log-loss} value across all folds. The value from the 
\emph{GridSearchCV} are different because, by default, it determines the 
"best" value of C as the one which corresponds to the average highest \emph{score} 
across all folds 
\footnote{See \emph{scoring} parameter in \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{documentation}. 
If the estimator provided exposes a \emph{score} method and a value for 
\emph{scoring} is not provided, then \emph{score} is used to determine the 
"best" value of C}.

We can modify the \emph{GridSearchCV} class by providing our own metric
for \emph{scoring}. The following code is a scorer that uses 
the smallest \emph{log-loss} value as its scoring metric.

\begin{verbatim}
scoring = make_scorer(
    log_loss,                # The sklearn implementation of log_loss
    greater_is_better=False, # A smaller log_loss is a better value
    needs_proba=True         # Calculating log_loss needs the probability predictions 
)
\end{verbatim}

This yields the following results:

\emph{grid\_lr.best\_estimator\_}: LogisticRegression(C=0.18188787878787877, penalty='l1', solver='liblinear')
\emph{grid\_lr.best\_score\_} (lowest log-loss): -0.5374067706086373

This value of \emph{C} matches my value in b \ref{1b}.

\end{document}
